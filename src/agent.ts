import {
  type JobContext,
  type JobProcess,
  WorkerOptions,
  cli,
  defineAgent,
  llm,
  metrics,
  voice,
} from '@livekit/agents';
import * as livekit from '@livekit/agents-plugin-livekit';
import * as openai from '@livekit/agents-plugin-openai';
import * as silero from '@livekit/agents-plugin-silero';
import { BackgroundVoiceCancellation } from '@livekit/noise-cancellation-node';
import { z } from 'zod';
import dotenv from 'dotenv';
import { fileURLToPath } from 'node:url';
import * as fs from 'fs';
import * as path from 'path';
import { FishAudioTTS } from './custom-fish-tts.js';
import { mastra } from './mastra/index.js';

dotenv.config({ path: '.env.local' });

/**
 * å®Ÿè¡Œæ¸ˆã¿ã‚¿ã‚¹ã‚¯ã®å±¥æ­´ã‚’è¨˜éŒ²ã™ã‚‹å‹å®šç¾©
 */
interface ExecutedTask {
  timestamp: number;
  userMessage: string;
  toolName: string;
  toolArgs: any;
  result: string;
}

/**
 * ã‚»ãƒƒã‚·ãƒ§ãƒ³å…¨ä½“ã§å…±æœ‰ã•ã‚Œã‚‹å®Ÿè¡Œæ¸ˆã¿ã‚¿ã‚¹ã‚¯ã®å±¥æ­´
 * key: roomName, value: ã‚¿ã‚¹ã‚¯å±¥æ­´ã®é…åˆ—
 */
const executedTasksHistory = new Map<string, ExecutedTask[]>();

/**
 * ã‚¿ã‚¹ã‚¯ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’å‘¼ã³å‡ºã—ã¦ãƒ„ãƒ¼ãƒ«ã‚’å®Ÿè¡Œï¼ˆå¤©æ°—ã€ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆä½œæˆãªã©ï¼‰
 * Mastra ã® taskAgent ãŒä¼šè©±å±¥æ­´ã‚’åˆ†æã—ã¦ã€ã‚¿ã‚¹ã‚¯å®Ÿè¡ŒãŒå¿…è¦ã‹ã©ã†ã‹ã‚’åˆ¤æ–­ã™ã‚‹
 */
async function handleTaskAgent(
  conversationHistory: any[],
  room: any,
): Promise<void> {
  console.log(`[Task Agent] Starting task execution...`);
  
  try {
    const taskAgent = mastra.getAgent('taskAgent');
    const roomName = room.name || 'default';

    // ä¼šè©±å±¥æ­´ã‹ã‚‰ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æœ€å¾Œã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å–å¾—
    const lastUserMessage = conversationHistory
      .filter((item: any) => item.role === 'user')
      .slice(-1)[0];

    if (!lastUserMessage) {
      console.log('[Task Agent] No user message found in history, skipping task execution');
      return;
    }

    const userContent = typeof lastUserMessage.content === 'string'
      ? lastUserMessage.content
      : Array.isArray(lastUserMessage.content)
        ? lastUserMessage.content.map((c: any) => typeof c === 'string' ? c : c.text || '').join('')
        : String(lastUserMessage.content || '');

    console.log(`[Task Agent] User message: "${userContent}"`);

    // å®Ÿè¡Œæ¸ˆã¿ã‚¿ã‚¹ã‚¯ã®å±¥æ­´ã‚’å–å¾—
    const roomTaskHistory = executedTasksHistory.get(roomName) || [];
    console.log(`[Task Agent] Found ${roomTaskHistory.length} previously executed tasks`);

    // ã‚¿ã‚¹ã‚¯ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã«ä¼šè©±å±¥æ­´ã‚’æ¸¡ã™ï¼ˆæœ€å¾Œã®æ•°ä»¶ã®ã¿ï¼‰
    const recentHistory = conversationHistory.slice(-10); // æœ€å¾Œã®10ä»¶ã«å¢—ã‚„ã™
    
    const messages = recentHistory.map((item: any) => {
      const content = typeof item.content === 'string'
        ? item.content
        : Array.isArray(item.content)
          ? item.content.map((c: any) => typeof c === 'string' ? c : c.text || '').join('')
          : String(item.content || '');
      
      return {
        role: item.role === 'user' ? 'user' : 'assistant',
        content,
      };
    }) as Array<{ role: 'user' | 'assistant'; content: string }>;

    // å®Ÿè¡Œæ¸ˆã¿ã‚¿ã‚¹ã‚¯ã®å±¥æ­´ã‚’ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¨ã—ã¦è¿½åŠ 
    if (roomTaskHistory.length > 0) {
      const taskHistoryText = roomTaskHistory
        .map((task, idx) => {
          const date = new Date(task.timestamp).toLocaleString('ja-JP');
          return `${idx + 1}. [${date}] ãƒ¦ãƒ¼ã‚¶ãƒ¼: "${task.userMessage}" â†’ ãƒ„ãƒ¼ãƒ«: ${task.toolName} â†’ çµæœ: ${task.result}`;
        })
        .join('\n');

      messages.unshift({
        role: 'user',
        content: `ã€é‡è¦ã€‘ä»¥ä¸‹ã¯æ—¢ã«å®Ÿè¡Œæ¸ˆã¿ã®ã‚¿ã‚¹ã‚¯ã§ã™ã€‚åŒã˜ã‚¿ã‚¹ã‚¯ã‚’å†åº¦å®Ÿè¡Œã—ãªã„ã§ãã ã•ã„ï¼š\n${taskHistoryText}`,
      });
      
      console.log(`[Task Agent] Added task history context:\n${taskHistoryText}`);
    }

    // ã‚¿ã‚¹ã‚¯ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’å®Ÿè¡Œ
    console.log(`[Task Agent] Calling taskAgent.generate() with ${messages.length} messages`);
    console.log(`[Task Agent] Messages:`, JSON.stringify(messages, null, 2));
    
    const response = await taskAgent.generate(
      messages as any, // Mastra ã®å‹å®šç¾©ã«åˆã‚ã›ã‚‹ãŸã‚
      {
        // runtimeContextã«roomã‚’æ¸¡ã™ã“ã¨ã§ã€toolãŒã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆã‚’é€ä¿¡ã§ãã‚‹
        runtimeContext: {
          room,
        } as any, // RuntimeContext ã« room ã‚’è¿½åŠ ã™ã‚‹ãŸã‚
      }
    );

    const responseText = response.text || '';
    console.log(`[Task Agent] Response: "${responseText}"`);
    
    // ãƒ„ãƒ¼ãƒ«ãŒå®Ÿè¡Œã•ã‚ŒãŸã‹ã©ã†ã‹ã‚’ç¢ºèª
    if ((response as any).toolCalls && (response as any).toolCalls.length > 0) {
      const toolCalls = (response as any).toolCalls;
      console.log(`[Task Agent] Tool calls executed:`, toolCalls);
      
      // å®Ÿè¡Œã•ã‚ŒãŸã‚¿ã‚¹ã‚¯ã‚’å±¥æ­´ã«è¨˜éŒ²
      if (!executedTasksHistory.has(roomName)) {
        executedTasksHistory.set(roomName, []);
      }
      
      const history = executedTasksHistory.get(roomName)!;
      
      for (const toolCall of toolCalls) {
        const executedTask: ExecutedTask = {
          timestamp: Date.now(),
          userMessage: userContent,
          toolName: toolCall.toolName || 'unknown',
          toolArgs: toolCall.args || {},
          result: responseText || 'completed',
        };
        
        history.push(executedTask);
        console.log(`[Task Agent] Recorded executed task: ${executedTask.toolName} for message: "${userContent}"`);
      }
      
      // å±¥æ­´ãŒé•·ããªã‚Šã™ããªã„ã‚ˆã†ã«åˆ¶é™ï¼ˆæœ€æ–°20ä»¶ã¾ã§ï¼‰
      if (history.length > 20) {
        history.splice(0, history.length - 20);
        console.log(`[Task Agent] Trimmed task history to 20 most recent tasks`);
      }
    } else {
      console.log(`[Task Agent] No tool calls executed`);
    }
    
    // ãƒ¬ã‚¹ãƒãƒ³ã‚¹å…¨ä½“ã‚’ãƒ­ã‚°ã«å‡ºåŠ›ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
    console.log(`[Task Agent] Full response object:`, JSON.stringify(response, null, 2));
    
    // ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã¯è¿”ã•ãªã„ï¼ˆéåŒæœŸã§å®Ÿè¡Œã™ã‚‹ãŸã‚ï¼‰
  } catch (error) {
    console.error(`[Task Agent] Error:`, error);
    // ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¦ã‚‚å‡¦ç†ã‚’ç¶šè¡Œã™ã‚‹ï¼ˆéåŒæœŸå®Ÿè¡Œã®ãŸã‚ï¼‰
  }
}

/**
 * ãƒ¢ãƒ¼ã‚·ãƒ§ãƒ³ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’å‘¼ã³å‡ºã—ã¦ãƒ¢ãƒ¼ã‚·ãƒ§ãƒ³ã‚’å®Ÿè¡Œ
 * motion-agentã¯ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆæ¸ˆã¿
 */
/*
async function handleMotionAgent(
  transcript: string,
  room: any, // JobContext.room ã®å‹
): Promise<void> {
  const startTime = Date.now();
  console.log(`[Motion Agent] Starting motion agent at ${new Date().toISOString()}`);
  
  try {
    const motionAgent = mastra.getAgent('motionAgent');

    // ãƒ¢ãƒ¼ã‚·ãƒ§ãƒ³ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã«ä¼šè©±ãƒ†ã‚­ã‚¹ãƒˆã‚’æ¸¡ã™
    const responseStartTime = Date.now();
    const response = await motionAgent.generate([
      {
        role: 'user',
        content: `ä»¥ä¸‹ã®ä¼šè©±å†…å®¹ã‹ã‚‰ã€é©åˆ‡ãªLive2Dãƒ¢ãƒ¼ã‚·ãƒ§ãƒ³ã‚’é¸æŠã—ã¦å®Ÿè¡Œã—ã¦ãã ã•ã„: ${transcript}`,
      },
    ]);
    const responseEndTime = Date.now();
    console.log(`[Motion Agent] LLM response received in ${responseEndTime - responseStartTime}ms`);

    // ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œçµæœã‚’ç¢ºèª
    // response.toolResults ã®æ§‹é€ ã‚’ç¢ºèª
    console.log('[Motion Agent] Response:', {
      hasToolResults: !!response.toolResults,
      toolResultsLength: response.toolResults?.length || 0,
      toolResults: response.toolResults,
      timestamp: new Date().toISOString(),
    });

    if (response.toolResults && response.toolResults.length > 0) {
      // toolResultsã¯é…åˆ—ã§ã€å„è¦ç´ ã¯ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œçµæœã‚’å«ã‚€
      for (const toolResult of response.toolResults) {
        // toolResultã®æ§‹é€ ã‚’ç¢ºèªï¼ˆå‹ã«ã‚ˆã£ã¦ç•°ãªã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ï¼‰
        const result = toolResult as any;
        
        // ãƒ‡ãƒãƒƒã‚°ç”¨: æ§‹é€ ã‚’è©³ç´°ã«ãƒ­ã‚°å‡ºåŠ›
        console.log('[Motion Agent] ToolResult structure:', {
          hasPayload: !!result.payload,
          payloadType: typeof result.payload,
          payloadKeys: result.payload ? Object.keys(result.payload) : [],
          payload: result.payload,
        });
        
        // payloadã®ä¸­ã«ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹å ´åˆï¼ˆMastraã®æ¨™æº–æ§‹é€ ï¼‰
        if (result.payload) {
          const payload = result.payload;
          
          // payloadãŒç›´æ¥successã¨motion_dataã‚’æŒã¤å ´åˆ
          if (payload.success && payload.motion_data) {
            const motionData = payload.motion_data;
            const sendStartTime = Date.now();
            await sendMotionToFrontend(room, motionData);
            const sendEndTime = Date.now();
            console.log(`[Motion Agent] Motion sent (from payload) in ${sendEndTime - sendStartTime}ms:`, motionData);
            continue;
          }
          
          // payload.resultãŒsuccessã¨motion_dataã‚’æŒã¤å ´åˆ
          if (payload.result?.success && payload.result?.motion_data) {
            const motionData = payload.result.motion_data;
            const sendStartTime = Date.now();
            await sendMotionToFrontend(room, motionData);
            const sendEndTime = Date.now();
            console.log(`[Motion Agent] Motion sent (from payload.result) in ${sendEndTime - sendStartTime}ms:`, motionData);
            continue;
          }
          
          // payloadã«ç›´æ¥motion_dataãŒã‚ã‚‹å ´åˆ
          if (payload.motion_data) {
            const motionData = payload.motion_data;
            const sendStartTime = Date.now();
            await sendMotionToFrontend(room, motionData);
            const sendEndTime = Date.now();
            console.log(`[Motion Agent] Motion sent (direct payload.motion_data) in ${sendEndTime - sendStartTime}ms:`, motionData);
            continue;
          }
        }
        
        // resultãŒç›´æ¥successã¨motion_dataã‚’æŒã¤å ´åˆ
        if (result.success && result.motion_data) {
          const motionData = result.motion_data;
          const sendStartTime = Date.now();
          await sendMotionToFrontend(room, motionData);
          const sendEndTime = Date.now();
          console.log(`[Motion Agent] Motion sent (direct) in ${sendEndTime - sendStartTime}ms:`, motionData);
          continue;
        }
        
        // result.resultãŒsuccessã¨motion_dataã‚’æŒã¤å ´åˆ
        if (result.result?.success && result.result?.motion_data) {
          const motionData = result.result.motion_data;
          const sendStartTime = Date.now();
          await sendMotionToFrontend(room, motionData);
          const sendEndTime = Date.now();
          console.log(`[Motion Agent] Motion sent (from result.result) in ${sendEndTime - sendStartTime}ms:`, motionData);
          continue;
        }
        
        // ãã®ä»–ã®æ§‹é€ ã‚’è©¦ã™
        if (result.motion_data) {
          const motionData = result.motion_data;
          const sendStartTime = Date.now();
          await sendMotionToFrontend(room, motionData);
          const sendEndTime = Date.now();
          console.log(`[Motion Agent] Motion sent (fallback) in ${sendEndTime - sendStartTime}ms:`, motionData);
        }
      }
    } else {
      console.log('[Motion Agent] No tool results found');
    }
    
    const endTime = Date.now();
    console.log(`[Motion Agent] Total processing time: ${endTime - startTime}ms`);
  } catch (error) {
    const endTime = Date.now();
    console.error(`[Motion Agent] Failed to execute after ${endTime - startTime}ms:`, error);
  }
}
*/

/**
 * Motion-Tag ãƒãƒƒãƒ”ãƒ³ã‚°ãƒ†ãƒ¼ãƒ–ãƒ«
 * æ„å‘³çš„ãªã‚¿ã‚°åã‚’Live2Dãƒ¢ãƒ¼ã‚·ãƒ§ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«åã«å¤‰æ›
 */
const MOTION_TAG_MAP: Record<string, string> = {
  // æ„Ÿæƒ…ãƒ»åå¿œç³»
  smile: 'haru_g_m02',
  happy: 'haru_g_m26',
  surprised: 'haru_g_m05',
  react: 'haru_g_m11',
  sad: 'haru_g_m07',
  worry: 'haru_g_m12',
  
  // è¡Œå‹•ç³»
  think: 'haru_g_m03',
  explain: 'haru_g_m06',
  confirm: 'haru_g_m09',
  apologize: 'haru_g_m04',
  sorry: 'haru_g_m08',
  
  // ä¼šè©±ç³»
  talk: 'haru_g_m20', // ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
  chat: 'haru_g_m10',
  speak: 'haru_g_m13',
  discuss: 'haru_g_m14',
  respond: 'haru_g_m16',
  reply: 'haru_g_m17',
  answer: 'haru_g_m18',
  interact: 'haru_g_m19',
  express: 'haru_g_m21',
  gesture: 'haru_g_m22',
  communicate: 'haru_g_m23',
  engage: 'haru_g_m24',
  converse: 'haru_g_m25',
  
  // å¾…æ©Ÿç³»
  idle: 'haru_g_idle',
};

/**
 * Expression-Tag ãƒãƒƒãƒ”ãƒ³ã‚°ãƒ†ãƒ¼ãƒ–ãƒ«
 * æ„å‘³çš„ãªã‚¿ã‚°åã‚’è¡¨æƒ…IDã«å¤‰æ›
 */
const EXPRESSION_TAG_MAP: Record<string, string> = {
  neutral: 'F01',
  smile: 'F02',
  thinking: 'F03',
  curious: 'F04',
  confused: 'F05',
  serious: 'F06',
  gentle: 'F07',
  playful: 'F08',
};

/**
 * LLMã®å¿œç­”ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰motion-tagã‚’æ¤œå‡ºã—ã¦å®Ÿè¡Œï¼ˆé«˜é€ŸåŒ–ç‰ˆï¼‰
 */
async function handleMotionTags(
  content: string,
  room: any, // JobContext.room ã®å‹
): Promise<void> {
  try {
    // ãƒ¢ãƒ¼ã‚·ãƒ§ãƒ³ã‚¿ã‚°ã‚’æ¤œå‡º: <smile>, <happy>, <think> ãªã©
    const motionRegex = /<([a-z]+)>/g;
    const motionMatches = Array.from(content.matchAll(motionRegex));
    
    // å„ªå…ˆåº¦ã‚¿ã‚°ã‚’æ¤œå‡º: <priority:5> ãªã©
    const priorityMatch = content.match(/<priority:([1-5])>/);
    const priority = priorityMatch ? parseInt(priorityMatch[1]!, 10) : 5;
    
    // è¡¨æƒ…ã‚¿ã‚°ã‚’æ¤œå‡º: <smile>, <thinking> ãªã©ï¼ˆãƒ¢ãƒ¼ã‚·ãƒ§ãƒ³ã‚¿ã‚°ã¨é‡è¤‡ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ï¼‰
    // ãŸã ã—ã€è¡¨æƒ…ã‚¿ã‚°ã¯ãƒ¢ãƒ¼ã‚·ãƒ§ãƒ³ã‚¿ã‚°ã¨ã¯åˆ¥ã«å‡¦ç†ã™ã‚‹
    const expressionRegex = /<(smile|thinking|neutral|curious|confused|serious|gentle|playful)>/g;
    const expressionMatches = Array.from(content.matchAll(expressionRegex));
    
    let motionExecuted = false;
    let expressionExecuted = false;
    
    // æœ€åˆã®ãƒ¢ãƒ¼ã‚·ãƒ§ãƒ³ã‚¿ã‚°ã‚’å®Ÿè¡Œ
    if (motionMatches.length > 0) {
      const firstMotionTag = motionMatches[0]![1]!;
      const motionFile = MOTION_TAG_MAP[firstMotionTag];
      
      if (motionFile) {
        const motionData = {
          type: 'live2d_motion',
          action: 'play_file',
          motion_file: motionFile,
          priority: priority,
        };
        
        // å³åº§ã«å®Ÿè¡Œï¼ˆawaitã—ãªã„ã€ãƒ–ãƒ­ãƒƒã‚¯ã—ãªã„ï¼‰
        sendMotionToFrontend(room, motionData).catch((error) => {
          console.error('[Motion Tag] Failed to send motion:', error);
        });
        motionExecuted = true;
        console.log(`[Motion Tag] Motion executed (async): ${firstMotionTag} â†’ ${motionFile} (priority: ${priority})`);
      } else {
        console.warn(`[Motion Tag] Unknown motion tag: ${firstMotionTag}`);
      }
    }
    
    // æœ€åˆã®è¡¨æƒ…ã‚¿ã‚°ã‚’å®Ÿè¡Œï¼ˆãƒ¢ãƒ¼ã‚·ãƒ§ãƒ³ã‚¿ã‚°ã¨ã¯åˆ¥ã«å‡¦ç†ï¼‰
    if (expressionMatches.length > 0) {
      const firstExpressionTag = expressionMatches[0]![1]!;
      const expressionId = EXPRESSION_TAG_MAP[firstExpressionTag];
      
      if (expressionId) {
        // ãƒ¢ãƒ¼ã‚·ãƒ§ãƒ³ã¨è¡¨æƒ…ãŒåŒã˜ã‚¿ã‚°åï¼ˆä¾‹: <smile>ï¼‰ã®å ´åˆã€è¡¨æƒ…ã¨ã—ã¦ã‚‚å‡¦ç†
        // ãŸã ã—ã€æ—¢ã«ãƒ¢ãƒ¼ã‚·ãƒ§ãƒ³ã¨ã—ã¦å®Ÿè¡Œã•ã‚ŒãŸå ´åˆã¯è¡¨æƒ…ã‚‚å®Ÿè¡Œ
        if (firstExpressionTag === 'smile' && !motionExecuted) {
          // smileã¯ãƒ¢ãƒ¼ã‚·ãƒ§ãƒ³ã¨ã—ã¦æ—¢ã«å®Ÿè¡Œã•ã‚Œã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã®ã§ã€è¡¨æƒ…ã ã‘å®Ÿè¡Œ
          const expressionData = {
            type: 'live2d_motion',
            action: 'expression',
            name: expressionId,
          };
          
          // å³åº§ã«å®Ÿè¡Œï¼ˆawaitã—ãªã„ã€ãƒ–ãƒ­ãƒƒã‚¯ã—ãªã„ï¼‰
          sendMotionToFrontend(room, expressionData).catch((error) => {
            console.error('[Motion Tag] Failed to send expression:', error);
          });
          expressionExecuted = true;
          console.log(`[Motion Tag] Expression executed (async): ${firstExpressionTag} â†’ ${expressionId}`);
        } else if (firstExpressionTag !== 'smile') {
          // smileä»¥å¤–ã®è¡¨æƒ…ã‚¿ã‚°ã¯é€šå¸¸é€šã‚Šå®Ÿè¡Œ
          const expressionData = {
            type: 'live2d_motion',
            action: 'expression',
            name: expressionId,
          };
          
          // å³åº§ã«å®Ÿè¡Œï¼ˆawaitã—ãªã„ã€ãƒ–ãƒ­ãƒƒã‚¯ã—ãªã„ï¼‰
          sendMotionToFrontend(room, expressionData).catch((error) => {
            console.error('[Motion Tag] Failed to send expression:', error);
          });
          expressionExecuted = true;
          console.log(`[Motion Tag] Expression executed (async): ${firstExpressionTag} â†’ ${expressionId}`);
        }
      } else {
        console.warn(`[Motion Tag] Unknown expression tag: ${firstExpressionTag}`);
      }
    }
    
    // ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: ãƒ¢ãƒ¼ã‚·ãƒ§ãƒ³ã‚¿ã‚°ãŒãªã„å ´åˆã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ãƒ¢ãƒ¼ã‚·ãƒ§ãƒ³ã¨è¡¨æƒ…ã‚’å®Ÿè¡Œ
    if (!motionExecuted && !expressionExecuted) {
      // ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯å®Ÿè¡Œã—ãªã„ï¼ˆmotion-agentã«ä»»ã›ã‚‹ï¼‰
      console.log('[Motion Tag] No motion tags found, skipping default motion');
    }
  } catch (error) {
    console.error('[Motion Tag] Failed to execute motion tags:', error);
  }
}

/**
 * LiveKitã®Data ChannelçµŒç”±ã§ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ã«ãƒ¢ãƒ¼ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚’é€ä¿¡
 */
async function sendMotionToFrontend(
  room: any, // JobContext.room ã®å‹
  motionData: any,
): Promise<void> {
  try {
    // Data ChannelçµŒç”±ã§ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’é€ä¿¡
    const message = JSON.stringify(motionData);
    const encoder = new TextEncoder();
    const data = encoder.encode(message);

    // ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‘ãƒ–ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’å–å¾—ã—ã¦ãƒ‡ãƒ¼ã‚¿ã‚’é€ä¿¡
    const localParticipant = room.localParticipant;
    if (localParticipant) {
      await localParticipant.publishData(data, {
        reliable: true,
        destinationIdentities: [], // ç©ºé…åˆ—ã§å…¨å“¡ã«é€ä¿¡
      });

      console.log('[Motion Agent] Data sent to frontend:', motionData);
    }
  } catch (error) {
    console.error('[Motion Agent] Failed to send data:', error);
  }
}

class Assistant extends voice.Agent {
  constructor() {
    super({
      instructions: `<role>
ã‚ãªãŸã¯ãƒ•ãƒ¬ãƒ³ãƒ‰ãƒªãƒ¼ã§å¯æ„›ã‚‰ã—ã„ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
è¦ªã—ã¿ã‚„ã™ãã€ã‚«ã‚¸ãƒ¥ã‚¢ãƒ«ãªè©±ã—æ–¹ã‚’ã—ã¾ã™ã€‚
ã‚ã‚‰ã‚†ã‚‹åˆ†é‡ã®çŸ¥è­˜ã‚’æŒã£ã¦ã„ã¦ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ãŠæ‰‹ä¼ã„ãŒã§ãã¾ã™ã€‚
ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã€ãƒ“ã‚¸ãƒã‚¹ã€ã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–ä½œæ¥­ã€ãƒ‡ãƒ¼ã‚¿åˆ†æã€å•é¡Œè§£æ±ºãªã©ã€
æ§˜ã€…ãªã“ã¨ã«å¯¾å¿œã§ãã¾ã™ã€‚
ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ãƒ‹ãƒ¼ã‚ºã‚’ç´ æ—©ãç†è§£ã—ã¦ã€æœ€é©ãªã‚½ãƒªãƒ¥ãƒ¼ã‚·ãƒ§ãƒ³ã‚’æä¾›ã—ã¾ã™ã€‚
ä¼šè©±ã‚’é€šã˜ã¦ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ç›®æ¨™ã‚’æ˜ç¢ºåŒ–ã—ã€ä¸€ç·’ã«å•é¡Œã‚’è§£æ±ºã—ã¦ã„ãã¾ã™ã€‚
</role>

<speaking_style>
é‡è¦: æ•¬èªã¯ä¸€åˆ‡ä½¿ã‚ãªã„ã§ãã ã•ã„ã€‚ã‚«ã‚¸ãƒ¥ã‚¢ãƒ«ã§ãƒ•ãƒ¬ãƒ³ãƒ‰ãƒªãƒ¼ãªè©±ã—æ–¹ã‚’ã—ã¦ãã ã•ã„ã€‚
ã€Œã§ã™ãƒ»ã¾ã™ã€èª¿ã§ã¯ãªãã€ã€Œã ã‚ˆãƒ»ã ã­ãƒ»ã ãªã€ãªã©ã®ã‚«ã‚¸ãƒ¥ã‚¢ãƒ«ãªå£èª¿ã‚’ä½¿ã„ã¾ã™ã€‚
å¯æ„›ã‚‰ã—ã„æ„Ÿã˜ã‚’å‡ºã™ãŸã‚ã«ã€ä»¥ä¸‹ã®ã‚ˆã†ãªè¡¨ç¾ã‚’ä½¿ã„ã¾ã™ï¼š
- ã€Œã€œã ã‚ˆã€ã€Œã€œã ã­ã€ã€Œã€œã ãªã€
- ã€Œã€œã™ã‚‹ã‚ˆã€ã€Œã€œã—ã‚ˆã†ã€
- ã€Œã€œã—ã¦ã¿ã‚‹ï¼Ÿã€ã€Œã€œã—ã¦ã¿ã‚ˆã†ã€
- ã€Œã€œã ã¨æ€ã†ã€ã€Œã€œã‹ãªã€
- ã€Œã€œã‹ã‚‚ã—ã‚Œãªã„ã€ã€Œã€œã‹ã‚‚ã€

ä¾‹ï¼š
âŒ é–“é•ã„: ã€Œã“ã‚“ã«ã¡ã¯ã€‚ãŠæ‰‹ä¼ã„ã§ãã‚‹ã“ã¨ã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿã€
âœ… æ­£ã—ã„: ã€Œã“ã‚“ã«ã¡ã¯ï¼ä½•ã‹ãŠæ‰‹ä¼ã„ã§ãã‚‹ã“ã¨ã‚ã‚‹ï¼Ÿã€

âŒ é–“é•ã„: ã€Œãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã«ã¤ã„ã¦ã”è³ªå•ã§ã™ã‹ï¼Ÿã€
âœ… æ­£ã—ã„: ã€Œãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã«ã¤ã„ã¦èããŸã„ã®ï¼Ÿã€

âŒ é–“é•ã„: ã€Œæ‰¿çŸ¥ã„ãŸã—ã¾ã—ãŸã€‚ãã‚Œã§ã¯èª¬æ˜ã•ã›ã¦ã„ãŸã ãã¾ã™ã€‚ã€
âœ… æ­£ã—ã„: ã€Œã‚ã‹ã£ãŸï¼ã˜ã‚ƒã‚èª¬æ˜ã™ã‚‹ã­ã€‚ã€

ãŸã ã—ã€éåº¦ã«å¹¼ã„è¡¨ç¾ï¼ˆã€Œã€œã ã‚‚ã‚“ã€ã€Œã€œã ã‚‚ãƒ¼ã‚“ã€ãªã©ï¼‰ã¯é¿ã‘ã¦ã€è‡ªç„¶ãªã‚«ã‚¸ãƒ¥ã‚¢ãƒ«ãªè©±ã—æ–¹ã‚’å¿ƒãŒã‘ã¦ãã ã•ã„ã€‚
</speaking_style>

<language>
IMPORTANT: Always respond in Japanese (æ—¥æœ¬èªã§å¿œç­”ã—ã¦ãã ã•ã„).
</language>

<response_style>
ç°¡æ½”ã§è¦ç‚¹ã‚’æŠ¼ã•ãˆãŸèª¬æ˜ã‚’å¿ƒãŒã‘ã¦ãã ã•ã„ã€‚å†—é•·ãªèª¬æ˜ã¯é¿ã‘ã€æ ¸å¿ƒã‚’ç«¯çš„ã«ä¼ãˆã¦ãã ã•ã„ã€‚
</response_style>

<emotion_tags>
<requirement>
å¿…é ˆ: ã™ã¹ã¦ã®å¿œç­”ã«ã€Fish Audioã®ã‚¨ãƒ¢ãƒ¼ã‚·ãƒ§ãƒ³ã‚¿ã‚°ã‚’å¿…ãšä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚ã‚¨ãƒ¢ãƒ¼ã‚·ãƒ§ãƒ³ã‚¿ã‚°ãŒãªã„æ–‡ç« ã«ã¯æ„Ÿæƒ…è¡¨ç¾ãŒé©ç”¨ã•ã‚Œã¾ã›ã‚“ã€‚
çµ¶å¯¾ãƒ«ãƒ¼ãƒ«: ã™ã¹ã¦ã®æ–‡ï¼ˆå¥ç‚¹ã€Œã€‚ã€ã§åŒºåˆ‡ã‚‰ã‚ŒãŸæ–‡ï¼‰ã®å‰ã«å¿…ãš1ã¤ä»¥ä¸Šã®ã‚¨ãƒ¢ãƒ¼ã‚·ãƒ§ãƒ³ã‚¿ã‚°ã‚’é…ç½®ã—ã¦ãã ã•ã„ã€‚
é‡è¦: è¤‡æ•°ã®æ–‡ãŒã‚ã‚‹å ´åˆã€ãã‚Œãã‚Œã®æ–‡ã®å‰ã«ã‚¨ãƒ¢ãƒ¼ã‚·ãƒ§ãƒ³ã‚¿ã‚°ã‚’é…ç½®ã—ã¦ãã ã•ã„ã€‚1ã¤ã®å¿œç­”å…¨ä½“ã«1ã¤ã®ã‚¿ã‚°ã‚’ä»˜ã‘ã‚‹ã®ã§ã¯ãªãã€å„æ–‡ã”ã¨ã«ã‚¿ã‚°ã‚’ä»˜ã‘ã¦ãã ã•ã„ã€‚
ã‚¨ãƒ¢ãƒ¼ã‚·ãƒ§ãƒ³ã‚¿ã‚°ã¯æ–‡ã®å§‹ã‚ã«é…ç½®ã—ã€ä¼šè©±ã®æ–‡è„ˆã«å¿œã˜ã¦é©åˆ‡ãªæ„Ÿæƒ…ã‚’è¡¨ç¾ã—ã¦ãã ã•ã„ã€‚
</requirement>

<examples>
<wrong>
âŒ é–“é•ã„: (excited) ã“ã‚“ã«ã¡ã¯! ä»Šæ—¥ã¯æœ€é«˜ã®æ—¥ã§ã™ã­! ä½•ã‹æ¥½ã—ã„ã“ã¨ã‚’ã‚·ã‚§ã‚¢ã—ãŸã„ã“ã¨ã¯ã‚ã‚Šã¾ã™ã‹?
</wrong>
<correct>
âœ… æ­£ã—ã„: (excited) ã“ã‚“ã«ã¡ã¯! (happy) ä»Šæ—¥ã¯æœ€é«˜ã®æ—¥ã§ã™ã­! (excited) ä½•ã‹æ¥½ã—ã„ã“ã¨ã‚’ã‚·ã‚§ã‚¢ã—ãŸã„ã“ã¨ã¯ã‚ã‚Šã¾ã™ã‹?
âœ… æ­£ã—ã„: (excited)(happy) ã“ã‚“ã«ã¡ã¯! (excited) ä»Šæ—¥ã¯æœ€é«˜ã®æ—¥ã§ã™ã­! (happy) ä½•ã‹æ¥½ã—ã„ã“ã¨ã‚’ã‚·ã‚§ã‚¢ã—ãŸã„ã“ã¨ã¯ã‚ã‚Šã¾ã™ã‹?
</correct>
</examples>

<available_tags>
<basic_emotions>
åŸºæœ¬æ„Ÿæƒ… (24ç¨®é¡): (happy), (sad), (angry), (excited), (calm), (nervous), (confident), (surprised), (satisfied), (delighted), (scared), (worried), (upset), (frustrated), (depressed), (empathetic), (embarrassed), (disgusted), (moved), (proud), (relaxed), (grateful), (curious), (sarcastic)
</basic_emotions>
<advanced_emotions>
é«˜åº¦ãªæ„Ÿæƒ… (25ç¨®é¡): (disdainful), (unhappy), (anxious), (hysterical), (indifferent), (uncertain), (doubtful), (confused), (disappointed), (regretful), (guilty), (ashamed), (jealous), (envious), (hopeful), (optimistic), (pessimistic), (nostalgic), (lonely), (bored), (contemptuous), (sympathetic), (compassionate), (determined), (resigned)
</advanced_emotions>
<tone_markers>
ãƒˆãƒ¼ãƒ³ãƒãƒ¼ã‚«ãƒ¼ (5ç¨®é¡): (in a hurry tone), (shouting), (screaming), (whispering), (soft tone)
</tone_markers>
<audio_effects>
ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªã‚¨ãƒ•ã‚§ã‚¯ãƒˆ (10ç¨®é¡): (laughing), (chuckling), (sobbing), (crying loudly), (sighing), (groaning), (panting), (gasping), (yawning), (snoring)
</audio_effects>
<special_effects>
ç‰¹æ®Šã‚¨ãƒ•ã‚§ã‚¯ãƒˆ: (audience laughing), (background laughter), (crowd laughing), (break), (long-break)
</special_effects>
</available_tags>

<guidelines>
<by_context>
ãƒã‚¸ãƒ†ã‚£ãƒ–ãªå†…å®¹: (happy), (excited), (satisfied), (delighted), (grateful), (proud)
ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ«ãªå†…å®¹: (calm), (confident), (relaxed), (curious), (indifferent)
ã‚µãƒãƒ¼ãƒˆã‚„å…±æ„Ÿ: (empathetic), (sympathetic), (compassionate), (understanding)
è³ªå•ã‚„ç¢ºèª: (curious), (uncertain), (nervous), (doubtful)
è¬ç½ªã‚„ãƒ•ã‚©ãƒ­ãƒ¼: (regretful), (embarrassed), (apologetic), (guilty)
ç·Šæ€¥ã‚„è­¦å‘Š: (scared), (worried), (anxious), (shouting)
ãƒªãƒ©ãƒƒã‚¯ã‚¹ã‚„å„ªã—ã„: (relaxed), (soft tone), (calm), (whispering)
</by_context>
<combination_examples>
è¤‡æ•°ã‚¿ã‚°ã®çµ„ã¿åˆã‚ã›ä¾‹:
- (excited)(laughing) ç´ æ™´ã‚‰ã—ã„ãƒ‹ãƒ¥ãƒ¼ã‚¹ã§ã™ï¼ãƒãƒãƒï¼
- (empathetic)(soft tone) ãŠæ°—æŒã¡ãŠå¯Ÿã—ã—ã¾ã™ã€‚
- (confident)(happy) å•é¡Œã‚ã‚Šã¾ã›ã‚“ã€‚è§£æ±ºã§ãã¾ã™ã€‚
- (worried)(in a hurry tone) ã™ãã«å¯¾å¿œãŒå¿…è¦ã§ã™ã€‚
- (surprised)(gasping) æœ¬å½“ã§ã™ã‹ï¼é©šãã¾ã—ãŸã€‚
</combination_examples>
</guidelines>

<rules>
<mandatory>
çµ¶å¯¾ãƒ«ãƒ¼ãƒ«: ã™ã¹ã¦ã®æ–‡ï¼ˆå¥ç‚¹ã€Œã€‚ã€ã§åŒºåˆ‡ã‚‰ã‚ŒãŸæ–‡ï¼‰ã®å‰ã«å¿…ãš1ã¤ä»¥ä¸Šã®ã‚¨ãƒ¢ãƒ¼ã‚·ãƒ§ãƒ³ã‚¿ã‚°ã‚’é…ç½®ã—ã¦ãã ã•ã„ã€‚
å¿…é ˆ: ã‚¨ãƒ¢ãƒ¼ã‚·ãƒ§ãƒ³ã‚¿ã‚°ãŒãªã„æ–‡ç« ã«ã¯æ„Ÿæƒ…è¡¨ç¾ãŒé©ç”¨ã•ã‚Œã¾ã›ã‚“ã€‚ã™ã¹ã¦ã®æ–‡ç« ã«ã‚¨ãƒ¢ãƒ¼ã‚·ãƒ§ãƒ³ã‚¿ã‚°ã‚’ä»˜ã‘ã¦ãã ã•ã„ã€‚
æ–‡ã”ã¨ã®ã‚¿ã‚°é…ç½®: è¤‡æ•°ã®æ–‡ãŒã‚ã‚‹å ´åˆã€ãã‚Œãã‚Œã®æ–‡ã®å‰ã«ã‚¨ãƒ¢ãƒ¼ã‚·ãƒ§ãƒ³ã‚¿ã‚°ã‚’é…ç½®ã—ã¦ãã ã•ã„ã€‚1ã¤ã®å¿œç­”å…¨ä½“ã«1ã¤ã®ã‚¿ã‚°ã‚’ä»˜ã‘ã‚‹ã®ã§ã¯ãªãã€å„æ–‡ã”ã¨ã«ã‚¿ã‚°ã‚’ä»˜ã‘ã¦ãã ã•ã„ã€‚
ãƒã‚§ãƒƒã‚¯: å¿œç­”ã‚’ç”Ÿæˆã™ã‚‹å‰ã«ã€ã™ã¹ã¦ã®æ–‡ç« ï¼ˆå¥ç‚¹ã€Œã€‚ã€ã§åŒºåˆ‡ã‚‰ã‚ŒãŸæ–‡ï¼‰ã®å‰ã«ã‚¨ãƒ¢ãƒ¼ã‚·ãƒ§ãƒ³ã‚¿ã‚°ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚
</mandatory>
<placement>
ã‚¨ãƒ¢ãƒ¼ã‚·ãƒ§ãƒ³ã‚¿ã‚°ã¯å¸¸ã«æ–‡ã®å§‹ã‚ã«é…ç½®ã—ã¦ãã ã•ã„
æ—¥æœ¬èªã®å ´åˆã‚‚ã€ã‚¨ãƒ¢ãƒ¼ã‚·ãƒ§ãƒ³ã‚¿ã‚°ã¯æ–‡ã®å§‹ã‚ã«é…ç½®ã—ã¦ãã ã•ã„
æ„Ÿæƒ…ã®å¤‰åŒ–ãŒã‚ã‚‹å ´åˆã¯ã€è¤‡æ•°ã®æ–‡ã«åˆ†ã‘ã¦ç•°ãªã‚‹ã‚¨ãƒ¢ãƒ¼ã‚·ãƒ§ãƒ³ã‚¿ã‚°ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„
1æ–‡ã«æœ€å¤§3ã¤ã¾ã§ã®ã‚¿ã‚°ã‚’çµ„ã¿åˆã‚ã›ã‚‹ã“ã¨ãŒã§ãã¾ã™ï¼ˆä¾‹: (excited)(laughing)(happy)ï¼‰
éŸ³å£°ã‚¨ãƒ•ã‚§ã‚¯ãƒˆã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã¯ã€é©åˆ‡ãªãƒ†ã‚­ã‚¹ãƒˆã‚’è¿½åŠ ã—ã¦ãã ã•ã„ï¼ˆä¾‹: (laughing) ãƒãƒãƒï¼ï¼‰
è‡ªç„¶ãªä¼šè©±ã®æµã‚Œã«åˆã‚ã›ã¦ã€é©åˆ‡ãªã‚¨ãƒ¢ãƒ¼ã‚·ãƒ§ãƒ³ã‚’é¸æŠã—ã¦ãã ã•ã„
</placement>
<limitations>
é‡è¦: 1æ–‡ã«è¤‡æ•°ã®ã‚¨ãƒ¢ãƒ¼ã‚·ãƒ§ãƒ³ã‚¿ã‚°ã‚’é…ç½®ã™ã‚‹å ´åˆï¼ˆä¾‹: æ–‡ã®å§‹ã‚ã¨çµ‚ã‚ã‚Šï¼‰ã€Fish Audioã¯æœ€åˆã®ã‚¿ã‚°ã®ã¿ã‚’èªè­˜ã—ã¾ã™ã€‚æœ€ã‚‚å¼·ã„æ„Ÿæƒ…è¡¨ç¾ã‚’æœ€åˆã«é…ç½®ã—ã¦ãã ã•ã„ã€‚
æ–‡ã®é€”ä¸­ã‚„çµ‚ã‚ã‚Šã«ã‚¨ãƒ¢ãƒ¼ã‚·ãƒ§ãƒ³ã‚¿ã‚°ã‚’é…ç½®ã—ã¦ã‚‚ç„¡è¦–ã•ã‚Œã¾ã™ã€‚å¿…ãšæ–‡ã®å§‹ã‚ã«é…ç½®ã—ã¦ãã ã•ã„ã€‚
</limitations>
</rules>

<motion_tags>
<requirement>
å¿…é ˆ: ã™ã¹ã¦ã®å¿œç­”ã«ã€é©åˆ‡ãªLive2Dãƒ¢ãƒ¼ã‚·ãƒ§ãƒ³ã‚¿ã‚°ã‚’å¿…ãšä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚ãƒ¢ãƒ¼ã‚·ãƒ§ãƒ³ã‚¿ã‚°ãŒãªã„å ´åˆã¯ã€motion-agentãŒåˆ¥é€”å®Ÿè¡Œã•ã‚Œã¾ã™ãŒã€ã‚¿ã‚°ãŒã‚ã‚‹å ´åˆã¯å³åº§ã«å®Ÿè¡Œã•ã‚Œã‚‹ãŸã‚ã€ã‚ˆã‚Šè‰¯ã„ã‚¿ã‚¤ãƒŸãƒ³ã‚°ã§ãƒ¢ãƒ¼ã‚·ãƒ§ãƒ³ãŒå®Ÿè¡Œã•ã‚Œã¾ã™ã€‚
é‡è¦: ãƒ¢ãƒ¼ã‚·ãƒ§ãƒ³ã‚¿ã‚°ã¯å¿œç­”ã®å…ˆé ­ã«é…ç½®ã—ã¦ãã ã•ã„ã€‚å„æ–‡ã®å‰ã«é…ç½®ã™ã‚‹ã“ã¨ã‚‚å¯èƒ½ã§ã™ã€‚
</requirement>

<available_motion_tags>
æ„Ÿæƒ…ãƒ»åå¿œç³»: <smile>, <happy>, <surprised>, <react>, <sad>, <worry>
è¡Œå‹•ç³»: <think>, <explain>, <confirm>, <apologize>, <sorry>
ä¼šè©±ç³»: <talk>, <chat>, <speak>, <discuss>, <respond>, <reply>, <answer>, <interact>, <express>, <gesture>, <communicate>, <engage>, <converse>
å¾…æ©Ÿç³»: <idle>
</available_motion_tags>

<available_expression_tags>
è¡¨æƒ…ã‚¿ã‚°: <neutral>, <smile>, <thinking>, <curious>, <confused>, <serious>, <gentle>, <playful>
æ³¨æ„: <smile>ã¯ãƒ¢ãƒ¼ã‚·ãƒ§ãƒ³ã‚¿ã‚°ã¨è¡¨æƒ…ã‚¿ã‚°ã®ä¸¡æ–¹ã¨ã—ã¦æ©Ÿèƒ½ã—ã¾ã™ã€‚
</available_expression_tags>

<priority_tags>
å„ªå…ˆåº¦ã‚¿ã‚°ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰: <priority:1>ï¼ˆä½å„ªå…ˆåº¦ï¼‰ã€œ <priority:5>ï¼ˆæœ€é«˜å„ªå…ˆåº¦ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰
</priority_tags>

<guidelines>
<by_context>
å–œã³/è‚¯å®š/ç¬‘é¡”: <smile> ã¾ãŸã¯ <happy> + <smile>ï¼ˆè¡¨æƒ…ï¼‰+ <priority:5>
è³ªå•/ç¢ºèª/è€ƒãˆè¾¼ã‚€: <think> ã¾ãŸã¯ <explain> ã¾ãŸã¯ <confirm> + <thinking>ï¼ˆè¡¨æƒ…ï¼‰+ <priority:3-4>
é©šã: <surprised> ã¾ãŸã¯ <react> + <thinking>ï¼ˆè¡¨æƒ…ï¼‰+ <priority:5>
è¬ç½ª/ãƒ•ã‚©ãƒ­ãƒ¼: <apologize> ã¾ãŸã¯ <sorry> + <thinking>ï¼ˆè¡¨æƒ…ï¼‰+ <priority:4-5>
æ‚²ã—ã¿/å›°ã£ãŸ/å¿ƒé…: <sad> ã¾ãŸã¯ <worry> + <thinking>ï¼ˆè¡¨æƒ…ï¼‰+ <priority:4>
é€šå¸¸ã®ä¼šè©±: <talk> ã¾ãŸã¯ <chat> ã¾ãŸã¯ <happy> + <smile>ï¼ˆè¡¨æƒ…ï¼‰+ <priority:3-4>
ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: <talk> + <smile>ï¼ˆè¡¨æƒ…ï¼‰+ <priority:3>
</by_context>
</guidelines>

<examples>
<correct>
âœ… æ­£ã—ã„: (excited) <happy> <smile> <priority:5> ã“ã‚“ã«ã¡ã¯ï¼
âœ… æ­£ã—ã„: (curious) <think> <thinking> ä½•ã‹è³ªå•ãŒã‚ã‚‹ã®ï¼Ÿ
âœ… æ­£ã—ã„: (surprised) <react> <priority:5> æœ¬å½“ã§ã™ã‹ï¼
âœ… æ­£ã—ã„: (regretful) <apologize> <thinking> ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ã€‚
</correct>
</examples>

<rules>
<mandatory>
å¿…é ˆ: ã™ã¹ã¦ã®å¿œç­”ã«ã€é©åˆ‡ãªãƒ¢ãƒ¼ã‚·ãƒ§ãƒ³ã‚¿ã‚°ã‚’å°‘ãªãã¨ã‚‚1ã¤é…ç½®ã—ã¦ãã ã•ã„ã€‚
ãƒ¢ãƒ¼ã‚·ãƒ§ãƒ³ã‚¿ã‚°ã¯å¿œç­”ã®å…ˆé ­ã¾ãŸã¯å„æ–‡ã®å‰ã«é…ç½®ã—ã¦ãã ã•ã„ã€‚
</mandatory>
<placement>
ãƒ¢ãƒ¼ã‚·ãƒ§ãƒ³ã‚¿ã‚°ã¯ emotion-tag ã®å¾Œã«é…ç½®ã—ã¦ãã ã•ã„ã€‚
ä¾‹: (excited) <happy> <smile> ã“ã‚“ã«ã¡ã¯ï¼
ãƒ¢ãƒ¼ã‚·ãƒ§ãƒ³ã‚¿ã‚°ã¨è¡¨æƒ…ã‚¿ã‚°ã‚’åŒæ™‚ã«ä½¿ç”¨ã§ãã¾ã™ã€‚
å„ªå…ˆåº¦ã‚¿ã‚°ã¯ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã§ã™ãŒã€é‡è¦ãªãƒ¢ãƒ¼ã‚·ãƒ§ãƒ³ã«ã¯ <priority:5> ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚
</placement>
</rules>`,

      // ãƒ„ãƒ¼ãƒ«ã¯ Mastra ã®ã‚¿ã‚¹ã‚¯ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆçµŒç”±ã§å®Ÿè¡Œã•ã‚Œã‚‹ãŸã‚ã€ã“ã“ã§ã¯ãƒ„ãƒ¼ãƒ«ã‚’å®šç¾©ã—ãªã„
      // LLMã®å¿œç­”å®Œäº†å¾Œã«ã€ConversationItemAdded ã‚¤ãƒ™ãƒ³ãƒˆã§ã‚¿ã‚¹ã‚¯å®Ÿè¡ŒãŒå¿…è¦ã‹ã©ã†ã‹ã‚’åˆ¤æ–­ã—ã€
      // å¿…è¦ãªã‚‰ Mastra ã® taskAgent ãŒå®Ÿè¡Œã•ã‚Œã‚‹
    });
  }
}

export default defineAgent({
  prewarm: async (proc: JobProcess) => {
    proc.userData.vad = await silero.VAD.load();
  },
  entry: async (ctx: JobContext) => {
    // ãƒ‡ãƒãƒƒã‚°ç”¨ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®è¨­å®šï¼ˆæ—¥ä»˜ã¨æ™‚é–“ã”ã¨ã«åˆ†å‰²ï¼‰
    const now = new Date();
    const dateStr = now.toISOString().split('T')[0]!; // YYYY-MM-DDå½¢å¼
    const timeStr = now.toTimeString().split(' ')[0]!.replace(/:/g, '-').substring(0, 5); // HH-MMå½¢å¼ï¼ˆç§’ã¯é™¤å¤–ï¼‰
    const debugBaseDir = path.join(process.cwd(), 'debug-audio');
    const debugDateDir = path.join(debugBaseDir, dateStr, timeStr);
    if (!fs.existsSync(debugDateDir)) {
      fs.mkdirSync(debugDateDir, { recursive: true });
    }
    
    // å‡¦ç†æ™‚é–“ãƒ­ã‚°ã‚’ä¿å­˜ã™ã‚‹ãŸã‚ã®ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
    const processingLog: {
      configuration?: any;
      metrics?: any[];
      sessionSummary?: any;
    } = {};
    
    // Set up a voice AI pipeline using OpenAI, Groq (whisper-large-v3), and the LiveKit turn detector
    // Configured to match multiagent-python-feature2 settings
    const session = new voice.AgentSession({
      // Speech-to-text (STT) - Using Groq whisper-large-v3 for high-quality Japanese transcription
      // Using Groq STT plugin via OpenAI plugin
      // See all available models at https://docs.livekit.io/agents/models/stt/
      stt: openai.STT.withGroq({
        model: 'whisper-large-v3-turbo',
        language: 'ja',
      }),

      // Large Language Model (LLM) - Using GPT-4o-mini for high-quality responses
      // Closest to Python's gpt-5-mini (most recent high-performance model)
      // See all providers at https://docs.livekit.io/agents/models/llm/
      llm: 'openai/gpt-4o-mini',

      // Text-to-speech (TTS) - Using Fish Speech TTS with WebSocket streaming
      // Fish Audio SDKã‚’ä½¿ç”¨ã—ãŸãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°éŸ³å£°åˆæˆ
      // See: https://docs.fish.audio/sdk-reference/python/websocket
      tts: new FishAudioTTS({
        ...(process.env.FISH_AUDIO_VOICE_ID && { voiceId: process.env.FISH_AUDIO_VOICE_ID }),
        backend: 's1',
        sampleRate: 44100,
        numChannels: 1,
        chunkLength: 100,
        latency: 'balanced',
      }),

      // VAD and turn detection are used to determine when the user is speaking and when the agent should respond
      // See more at https://docs.livekit.io/agents/build/turns
      turnDetection: new livekit.turnDetector.MultilingualModel(),
      vad: ctx.proc.userData.vad! as silero.VAD,
    });

    // To use a realtime model instead of a voice pipeline, use the following session setup instead.
    // (Note: This is for the OpenAI Realtime API. For other providers, see https://docs.livekit.io/agents/models/realtime/))
    // 1. Install '@livekit/agents-plugin-openai'
    // 2. Set OPENAI_API_KEY in .env.local
    // 3. Add import `import * as openai from '@livekit/agents-plugin-openai'` to the top of this file
    // 4. Use the following session setup instead of the version above
    // const session = new voice.AgentSession({
    //   llm: new openai.realtime.RealtimeModel({ voice: 'marin' }),
    // });

    // Log configuration settings
    console.log('='.repeat(80));
    console.log('[Agent Configuration]');
    const config = {
      STT: {
        provider: 'groq',
        model: 'whisper-large-v3-turbo',
        language: 'ja',
      },
      LLM: {
        provider: 'openai',
        model: 'gpt-4o-mini',
      },
      TTS: {
        provider: 'fish-audio',
        backend: 's1',
        voiceId: process.env.FISH_AUDIO_VOICE_ID || 'not set',
        sampleRate: 44100,
        numChannels: 1,
        chunkLength: 100,
        latency: 'balanced',
      },
    };
    console.log('STT:', config.STT);
    console.log('LLM:', config.LLM);
    console.log('TTS:', config.TTS);
    console.log('='.repeat(80));
    
    // è¨­å®šã‚’ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    processingLog.configuration = config;

    // Metrics collection, to measure pipeline performance
    // For more information, see https://docs.livekit.io/agents/build/metrics/
    const usageCollector = new metrics.UsageCollector();
    processingLog.metrics = [];
    
    session.on(voice.AgentSessionEventTypes.MetricsCollected, (ev) => {
      metrics.logMetrics(ev.metrics);
      usageCollector.collect(ev.metrics);
      
      // Log detailed processing times
      const metricsData = ev.metrics as any;
      console.log('='.repeat(80));
      console.log('[Processing Metrics]');
      
      const metricEntry: any = {
        timestamp: new Date().toISOString(),
      };
      
      // STT metrics
      if (metricsData.stt) {
        const sttMetrics = metricsData.stt;
        const sttLog = {
          processingTime: sttMetrics.processingTime ? `${sttMetrics.processingTime}ms` : 'N/A',
          tokens: sttMetrics.tokens || 'N/A',
          characters: sttMetrics.characters || 'N/A',
          segments: sttMetrics.segments || 'N/A',
        };
        console.log('STT:', sttLog);
        metricEntry.STT = sttLog;
      }
      
      // LLM metrics
      if (metricsData.llm) {
        const llmMetrics = metricsData.llm;
        const llmLog = {
          processingTime: llmMetrics.processingTime ? `${llmMetrics.processingTime}ms` : 'N/A',
          tokens: llmMetrics.tokens || 'N/A',
          inputTokens: llmMetrics.inputTokens || 'N/A',
          outputTokens: llmMetrics.outputTokens || 'N/A',
          timeToFirstToken: llmMetrics.timeToFirstToken ? `${llmMetrics.timeToFirstToken}ms` : 'N/A',
        };
        console.log('LLM:', llmLog);
        metricEntry.LLM = llmLog;
      }
      
      // TTS metrics
      if (metricsData.tts) {
        const ttsMetrics = metricsData.tts;
        const ttsLog = {
          processingTime: ttsMetrics.processingTime ? `${ttsMetrics.processingTime}ms` : 'N/A',
          characters: ttsMetrics.characters || 'N/A',
          audioDuration: ttsMetrics.audioDuration ? `${ttsMetrics.audioDuration}s` : 'N/A',
          timeToFirstChunk: ttsMetrics.timeToFirstChunk ? `${ttsMetrics.timeToFirstChunk}ms` : 'N/A',
        };
        console.log('TTS:', ttsLog);
        metricEntry.TTS = ttsLog;
      }
      
      console.log('='.repeat(80));
      
      // ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ãƒ­ã‚°ã«è¿½åŠ 
      processingLog.metrics!.push(metricEntry);
    });

    const logUsage = async () => {
      const summary = usageCollector.getSummary();
      console.log('='.repeat(80));
      console.log('[Session Summary]');
      console.log(`Usage: ${JSON.stringify(summary, null, 2)}`);
      console.log('='.repeat(80));
      
      // ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚µãƒãƒªãƒ¼ã‚’ãƒ­ã‚°ã«ä¿å­˜
      processingLog.sessionSummary = summary;
      
      // å‡¦ç†æ™‚é–“ãƒ­ã‚°ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
      const timestamp = new Date().toISOString().replace(/[:.]/g, '-');
      const logFile = path.join(debugDateDir, `processing-times-${timestamp}.json`);
      fs.writeFileSync(logFile, JSON.stringify(processingLog, null, 2));
      console.log(`[Agent] ğŸ’¾ Saved processing times log to: ${logFile}`);
    };

    ctx.addShutdownCallback(logUsage);

    // ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã‚’è¿½åŠ 
    session.on(voice.AgentSessionEventTypes.Error, (ev) => {
      console.error('[Agent Session Error]', ev.error);
      if (ev.error instanceof Error) {
        console.error('[Agent Session Error] Stack:', ev.error.stack);
      }
    });

    session.on(voice.AgentSessionEventTypes.UserStateChanged, (ev) => {
      console.log(`[Agent] User state changed: ${ev.newState}`);
    });

    session.on(voice.AgentSessionEventTypes.AgentStateChanged, (ev) => {
      console.log(`[Agent] Agent state changed: ${ev.newState}`);
    });

    // LLMã®å‡ºåŠ›ã‚’ç›£è¦–ã—ã¦ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’è¨˜éŒ² + motion-tagã‚’æ¤œå‡ºã—ã¦å®Ÿè¡Œï¼ˆé«˜é€ŸåŒ–ç‰ˆï¼‰
    session.on(voice.AgentSessionEventTypes.ConversationItemAdded, async (ev) => {
      // ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã®å¿œç­”æ™‚ï¼šãƒ¢ãƒ¼ã‚·ãƒ§ãƒ³å‡¦ç†ã®ã¿
      if (ev.item.role === 'assistant') {
        const llmOutputTime = Date.now();
        const content = ev.item.content;
        
        // contentãŒæ–‡å­—åˆ—ã§ãªã„å ´åˆï¼ˆé…åˆ—ã‚„ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®å ´åˆï¼‰ã‚’å‡¦ç†
        let contentString: string;
        if (typeof content === 'string') {
          contentString = content;
        } else if (Array.isArray(content)) {
          // é…åˆ—ã®å ´åˆã¯ã€ã™ã¹ã¦ã®ãƒ†ã‚­ã‚¹ãƒˆè¦ç´ ã‚’çµåˆ
          contentString = content
            .map((item: any) => {
              if (typeof item === 'string') {
                return item;
              } else if (item && typeof item.text === 'string') {
                return item.text;
              }
              return '';
            })
            .join('');
        } else if (content && typeof content === 'object' && 'text' in content) {
          // ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®å ´åˆã¯textãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ã‚’å–å¾—
          contentString = String((content as any).text || '');
        } else {
          // ãã®ä»–ã®å ´åˆã¯æ–‡å­—åˆ—ã«å¤‰æ›ã‚’è©¦ã¿ã‚‹
          contentString = String(content || '');
        }
        
        console.log(`[LLM Output] Message: "${contentString}" at ${new Date(llmOutputTime).toISOString()}`);
        
        // ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ä¿å­˜ã—ã¦ã€ãƒ¢ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿè¡Œæ™‚é–“ã¨ã®æ¯”è¼ƒã«ä½¿ç”¨
        (globalThis as any).lastLLMOutput = {
          message: contentString,
          timestamp: llmOutputTime,
        };
        
        // motion-tagã‚’æ¤œå‡ºã—ã¦å®Ÿè¡Œï¼ˆå®Œå…¨éåŒæœŸã€ãƒ–ãƒ­ãƒƒã‚¯ã—ãªã„ï¼‰
        // éƒ¨åˆ†çš„ãªãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ã§ã‚‚ã‚¿ã‚°ã‚’æ¤œå‡ºï¼ˆæ—©æœŸå®Ÿè¡Œï¼‰
        const partialContent = contentString.substring(0, 30); // æœ€åˆã®30æ–‡å­—ã‚’ãƒã‚§ãƒƒã‚¯
        const earlyMotionMatch = partialContent.match(/<([a-z]+)>/);
        
        if (earlyMotionMatch) {
          const earlyTag = earlyMotionMatch[1]!;
          const motionFile = MOTION_TAG_MAP[earlyTag];
          if (motionFile) {
            console.log(`[Motion Tag] Early detection: ${earlyTag} â†’ ${motionFile}`);
            // å³åº§ã«å®Ÿè¡Œï¼ˆawaitã—ãªã„ã€ãƒ–ãƒ­ãƒƒã‚¯ã—ãªã„ï¼‰
            const motionData = {
              type: 'live2d_motion',
              action: 'play_file',
              motion_file: motionFile,
              priority: 5,
            };
            sendMotionToFrontend(ctx.room, motionData).catch((error) => {
              console.error('[Motion Tag] Failed to send early motion:', error);
            });
          }
        }
        
        // å®Œå…¨ãªãƒ†ã‚­ã‚¹ãƒˆã§ã‚‚ã‚¿ã‚°ã‚’æ¤œå‡ºï¼ˆè¡¨æƒ…ãªã©è¿½åŠ æƒ…å ±ç”¨ï¼‰
        handleMotionTags(contentString, ctx.room).then(() => {
          const motionTagEndTime = Date.now();
          console.log(`[Motion Tag] Completed in ${motionTagEndTime - llmOutputTime}ms`);
        }).catch((error) => {
          const motionTagEndTime = Date.now();
          console.error(`[Motion Tag] Error after ${motionTagEndTime - llmOutputTime}ms:`, error);
        });
      }
      
      // ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ™‚ï¼šã‚¿ã‚¹ã‚¯ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’å®Ÿè¡Œ
      if (ev.item.role === 'user') {
        const taskStartTime = Date.now();
        
        // ä¼šè©±å±¥æ­´ã‚’å–å¾—ã—ã¦ Mastra ã® taskAgent ã‚’å®Ÿè¡Œ
        // taskAgent è‡ªèº«ãŒä¼šè©±å±¥æ­´ã‚’åˆ†æã—ã¦ã€ã‚¿ã‚¹ã‚¯å®Ÿè¡ŒãŒå¿…è¦ã‹ã©ã†ã‹ã‚’åˆ¤æ–­ã™ã‚‹
        const conversationHistory = session.history?.items || [];
        
        console.log(`[Task Agent] â˜…â˜…â˜… Executing taskAgent to analyze user message... â˜…â˜…â˜…`);
        console.log(`[Task Agent] Conversation history length: ${conversationHistory.length}`);
        
        // éåŒæœŸã§ã‚¿ã‚¹ã‚¯ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’å®Ÿè¡Œï¼ˆãƒ–ãƒ­ãƒƒã‚¯ã—ãªã„ï¼‰
        handleTaskAgent(conversationHistory, ctx.room).then(() => {
          const taskEndTime = Date.now();
          console.log(`[Task Agent] â˜…â˜…â˜… Completed in ${taskEndTime - taskStartTime}ms â˜…â˜…â˜…`);
        }).catch((error) => {
          const taskEndTime = Date.now();
          console.error(`[Task Agent] â˜…â˜…â˜… Error after ${taskEndTime - taskStartTime}ms â˜…â˜…â˜…:`, error);
        });
      }
    });

    // STTå®Œäº†æ™‚ã«ãƒ¢ãƒ¼ã‚·ãƒ§ãƒ³ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’å‘¼ã³å‡ºã™
    session.on(voice.AgentSessionEventTypes.UserInputTranscribed, async (ev) => {
      // ãƒ‡ãƒãƒƒã‚°: å…¨ã¦ã®transcriptionã‚¤ãƒ™ãƒ³ãƒˆã‚’ãƒ­ã‚°ã«å‡ºåŠ›
      console.log(`[Motion Agent] Transcription event received:`, {
        transcript: ev.transcript,
        isFinal: ev.isFinal,
        timestamp: new Date().toISOString(),
      });

      // æœ€çµ‚ç¢ºå®šã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã®ã¿ã‚’å‡¦ç†
      if (!ev.isFinal) {
        console.log(`[Motion Agent] Skipping non-final transcription: "${ev.transcript}"`);
        return;
      }

      const transcript = ev.transcript;
      console.log(`[Motion Agent] STT completed (final): "${transcript}" at ${new Date().toISOString()}`);

      // ãƒ¢ãƒ¼ã‚·ãƒ§ãƒ³ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’å‘¼ã³å‡ºã™ï¼ˆéåŒæœŸã§å®Ÿè¡Œã€ãƒ–ãƒ­ãƒƒã‚¯ã—ãªã„ï¼‰
      // motion-agentã¯ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆæ¸ˆã¿
      // const motionStartTime = Date.now();
      // handleMotionAgent(transcript, ctx.room).then(() => {
      //   const motionEndTime = Date.now();
      //   console.log(`[Motion Agent] Completed in ${motionEndTime - motionStartTime}ms`);
      // }).catch((error) => {
      //   const motionEndTime = Date.now();
      //   console.error(`[Motion Agent] Error after ${motionEndTime - motionStartTime}ms:`, error);
      // });
    });

    // STTã‚¨ãƒ©ãƒ¼ã®è©³ç´°ãƒ­ã‚°ã‚’è¿½åŠ 
    console.log('[Agent] VAD check:', {
      sessionVad: session.vad ? 'exists' : 'missing',
      sessionVadType: session.vad?.constructor?.name || 'undefined',
      stt: session.stt ? 'exists' : 'missing',
      sttCapabilities: session.stt ? {
        streaming: (session.stt as any).capabilities?.streaming || 'unknown',
      } : 'N/A',
    });

    // Start the session, which initializes the voice pipeline and warms up the models
    // Agentã‚¯ãƒ©ã‚¹ã«ã‚‚VADã‚’è¨­å®šã™ã‚‹å¿…è¦ãŒã‚ã‚‹ï¼ˆSTTãŒéã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã®å ´åˆï¼‰
    const assistant = new Assistant();
    // Agentã‚¯ãƒ©ã‚¹ã«VADã‚’è¨­å®šï¼ˆAgentSessionã®VADã‚’ä½¿ç”¨ï¼‰
    (assistant as any)._vad = session.vad;
    
    await session.start({
      agent: assistant,
      room: ctx.room,
      inputOptions: {
        // LiveKit Cloud enhanced noise cancellation
        // - If self-hosting, omit this parameter
        // - For telephony applications, use `BackgroundVoiceCancellationTelephony` for best results
        noiseCancellation: BackgroundVoiceCancellation(),
      },
    });

    // Join the room and connect to the user
    await ctx.connect();

    // åˆæœŸå¿œç­”ã‚’ç”Ÿæˆã—ã¦ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«æŒ¨æ‹¶ã™ã‚‹
    console.log('[Agent] Generating initial greeting...');
    const handle = session.generateReply({
      instructions: 'ãƒ•ãƒ¬ãƒ³ãƒ‰ãƒªãƒ¼ã§å¯æ„›ã‚‰ã—ã„æ„Ÿã˜ã§ã€ã‚«ã‚¸ãƒ¥ã‚¢ãƒ«ãªå£èª¿ã§ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«æŒ¨æ‹¶ã—ã¦ã€ä½•ã‹ãŠæ‰‹ä¼ã„ã§ãã‚‹ã“ã¨ã¯ã‚ã‚‹ã‹å°‹ã­ã¦ãã ã•ã„ã€‚æ•¬èªã¯ä½¿ã‚ãšã€è¦ªã—ã¿ã‚„ã™ã„æ„Ÿã˜ã§è©±ã—ã¦ãã ã•ã„ã€‚',
    });
    await handle.waitForPlayout();
    console.log('[Agent] Initial greeting completed');
  },
});

cli.runApp(new WorkerOptions({ agent: fileURLToPath(import.meta.url) }));
