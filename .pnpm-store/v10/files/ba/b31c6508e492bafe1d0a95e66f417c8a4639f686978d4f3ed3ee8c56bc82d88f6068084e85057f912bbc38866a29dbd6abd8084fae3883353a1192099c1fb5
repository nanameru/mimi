{"version":3,"sources":["../../src/inference/llm.ts"],"sourcesContent":["// SPDX-FileCopyrightText: 2025 LiveKit, Inc.\n//\n// SPDX-License-Identifier: Apache-2.0\nimport OpenAI from 'openai';\nimport {\n  APIConnectionError,\n  APIStatusError,\n  APITimeoutError,\n  DEFAULT_API_CONNECT_OPTIONS,\n  toError,\n} from '../index.js';\nimport * as llm from '../llm/index.js';\nimport type { APIConnectOptions } from '../types.js';\nimport { type AnyString, createAccessToken } from './utils.js';\n\nconst DEFAULT_BASE_URL = 'https://agent-gateway.livekit.cloud/v1';\n\nexport type OpenAIModels =\n  | 'openai/gpt-5'\n  | 'openai/gpt-5-mini'\n  | 'openai/gpt-5-nano'\n  | 'openai/gpt-4.1'\n  | 'openai/gpt-4.1-mini'\n  | 'openai/gpt-4.1-nano'\n  | 'openai/gpt-4o'\n  | 'openai/gpt-4o-mini'\n  | 'openai/gpt-oss-120b';\n\nexport type GoogleModels = 'google/gemini-2.0-flash-lite';\n\nexport type QwenModels = 'qwen/qwen3-235b-a22b-instruct';\n\nexport type KimiModels = 'moonshotai/kimi-k2-instruct';\n\nexport type DeepSeekModels = 'deepseek-ai/deepseek-v3';\n\ntype ChatCompletionPredictionContentParam = OpenAI.Chat.Completions.ChatCompletionPredictionContent;\ntype WebSearchOptions = OpenAI.Chat.Completions.ChatCompletionCreateParams.WebSearchOptions;\ntype ToolChoice = OpenAI.Chat.Completions.ChatCompletionCreateParams['tool_choice'];\ntype Verbosity = 'low' | 'medium' | 'high';\n\nexport interface ChatCompletionOptions extends Record<string, unknown> {\n  frequency_penalty?: number;\n  logit_bias?: Record<string, number>;\n  logprobs?: boolean;\n  max_completion_tokens?: number;\n  max_tokens?: number;\n  metadata?: Record<string, string>;\n  modalities?: Array<'text' | 'audio'>;\n  n?: number;\n  parallel_tool_calls?: boolean;\n  prediction?: ChatCompletionPredictionContentParam | null;\n  presence_penalty?: number;\n  prompt_cache_key?: string;\n  reasoning_effort?: 'minimal' | 'low' | 'medium' | 'high';\n  safety_identifier?: string;\n  seed?: number;\n  service_tier?: 'auto' | 'default' | 'flex' | 'scale' | 'priority';\n  stop?: string | string[];\n  store?: boolean;\n  temperature?: number;\n  top_logprobs?: number;\n  top_p?: number;\n  user?: string;\n  verbosity?: Verbosity;\n  web_search_options?: WebSearchOptions;\n\n  // livekit-typed arguments\n  tool_choice?: ToolChoice;\n  // TODO(brian): support response format\n  // response_format?: OpenAI.Chat.Completions.ChatCompletionCreateParams['response_format']\n}\n\nexport type LLMModels =\n  | OpenAIModels\n  | GoogleModels\n  | QwenModels\n  | KimiModels\n  | DeepSeekModels\n  | AnyString;\n\nexport interface InferenceLLMOptions {\n  model: LLMModels;\n  provider?: string;\n  baseURL: string;\n  apiKey: string;\n  apiSecret: string;\n  modelOptions: ChatCompletionOptions;\n}\n\nexport interface GatewayOptions {\n  apiKey: string;\n  apiSecret: string;\n}\n\n/**\n * Livekit Cloud Inference LLM\n */\nexport class LLM extends llm.LLM {\n  private client: OpenAI;\n  private opts: InferenceLLMOptions;\n\n  constructor(opts: {\n    model: LLMModels;\n    provider?: string;\n    baseURL?: string;\n    apiKey?: string;\n    apiSecret?: string;\n    modelOptions?: InferenceLLMOptions['modelOptions'];\n  }) {\n    super();\n\n    const { model, provider, baseURL, apiKey, apiSecret, modelOptions } = opts;\n\n    const lkBaseURL = baseURL || process.env.LIVEKIT_INFERENCE_URL || DEFAULT_BASE_URL;\n    const lkApiKey = apiKey || process.env.LIVEKIT_INFERENCE_API_KEY || process.env.LIVEKIT_API_KEY;\n    if (!lkApiKey) {\n      throw new Error('apiKey is required: pass apiKey or set LIVEKIT_API_KEY');\n    }\n\n    const lkApiSecret =\n      apiSecret || process.env.LIVEKIT_INFERENCE_API_SECRET || process.env.LIVEKIT_API_SECRET;\n    if (!lkApiSecret) {\n      throw new Error('apiSecret is required: pass apiSecret or set LIVEKIT_API_SECRET');\n    }\n\n    this.opts = {\n      model,\n      provider,\n      baseURL: lkBaseURL,\n      apiKey: lkApiKey,\n      apiSecret: lkApiSecret,\n      modelOptions: modelOptions || {},\n    };\n\n    this.client = new OpenAI({\n      baseURL: this.opts.baseURL,\n      apiKey: '', // leave a temporary empty string to avoid OpenAI complain about missing key\n      timeout: 15000,\n    });\n  }\n\n  label(): string {\n    return 'inference.LLM';\n  }\n\n  get model(): string {\n    return this.opts.model;\n  }\n\n  static fromModelString(modelString: string): LLM {\n    return new LLM({ model: modelString });\n  }\n\n  chat({\n    chatCtx,\n    toolCtx,\n    connOptions = DEFAULT_API_CONNECT_OPTIONS,\n    parallelToolCalls,\n    toolChoice,\n    // TODO(AJS-270): Add response_format parameter support\n    extraKwargs,\n  }: {\n    chatCtx: llm.ChatContext;\n    toolCtx?: llm.ToolContext;\n    connOptions?: APIConnectOptions;\n    parallelToolCalls?: boolean;\n    toolChoice?: llm.ToolChoice;\n    // TODO(AJS-270): Add responseFormat parameter\n    extraKwargs?: Record<string, unknown>;\n  }): LLMStream {\n    let modelOptions: Record<string, unknown> = { ...(extraKwargs || {}) };\n\n    parallelToolCalls =\n      parallelToolCalls !== undefined\n        ? parallelToolCalls\n        : this.opts.modelOptions.parallel_tool_calls;\n\n    if (toolCtx && Object.keys(toolCtx).length > 0 && parallelToolCalls !== undefined) {\n      modelOptions.parallel_tool_calls = parallelToolCalls;\n    }\n\n    toolChoice = toolChoice !== undefined ? toolChoice : this.opts.modelOptions.tool_choice;\n    if (toolChoice) {\n      modelOptions.tool_choice = toolChoice;\n    }\n\n    // TODO(AJS-270): Add response_format support here\n\n    modelOptions = { ...modelOptions, ...this.opts.modelOptions };\n\n    return new LLMStream(this, {\n      model: this.opts.model,\n      provider: this.opts.provider,\n      client: this.client,\n      chatCtx,\n      toolCtx,\n      connOptions,\n      modelOptions,\n      gatewayOptions: {\n        apiKey: this.opts.apiKey,\n        apiSecret: this.opts.apiSecret,\n      },\n    });\n  }\n}\n\nexport class LLMStream extends llm.LLMStream {\n  private model: LLMModels;\n  private provider?: string;\n  private providerFmt: llm.ProviderFormat;\n  private client: OpenAI;\n  private modelOptions: Record<string, unknown>;\n\n  private gatewayOptions?: GatewayOptions;\n  private toolCallId?: string;\n  private toolIndex?: number;\n  private fncName?: string;\n  private fncRawArguments?: string;\n\n  constructor(\n    llm: LLM,\n    {\n      model,\n      provider,\n      client,\n      chatCtx,\n      toolCtx,\n      gatewayOptions,\n      connOptions,\n      modelOptions,\n      providerFmt,\n    }: {\n      model: LLMModels;\n      provider?: string;\n      client: OpenAI;\n      chatCtx: llm.ChatContext;\n      toolCtx?: llm.ToolContext;\n      gatewayOptions?: GatewayOptions;\n      connOptions: APIConnectOptions;\n      modelOptions: Record<string, any>;\n      providerFmt?: llm.ProviderFormat;\n    },\n  ) {\n    super(llm, { chatCtx, toolCtx, connOptions });\n    this.client = client;\n    this.gatewayOptions = gatewayOptions;\n    this.provider = provider;\n    this.providerFmt = providerFmt || 'openai';\n    this.modelOptions = modelOptions;\n    this.model = model;\n  }\n\n  protected async run(): Promise<void> {\n    // current function call that we're waiting for full completion (args are streamed)\n    // (defined inside the run method to make sure the state is reset for each run/attempt)\n    let retryable = true;\n    this.toolCallId = this.fncName = this.fncRawArguments = this.toolIndex = undefined;\n\n    try {\n      const messages = (await this.chatCtx.toProviderFormat(\n        this.providerFmt,\n      )) as OpenAI.ChatCompletionMessageParam[];\n\n      const tools = this.toolCtx\n        ? Object.entries(this.toolCtx).map(([name, func]) => ({\n            type: 'function' as const,\n            function: {\n              name,\n              description: func.description,\n              parameters: llm.toJsonSchema(\n                func.parameters,\n              ) as unknown as OpenAI.Chat.Completions.ChatCompletionTool['function']['parameters'],\n            },\n          }))\n        : undefined;\n\n      const requestOptions: Record<string, unknown> = { ...this.modelOptions };\n      if (!tools) {\n        delete requestOptions.tool_choice;\n      }\n\n      // Dynamically set the access token for the LiveKit Agent Gateway API\n      if (this.gatewayOptions) {\n        this.client.apiKey = await createAccessToken(\n          this.gatewayOptions.apiKey,\n          this.gatewayOptions.apiSecret,\n        );\n      }\n\n      if (this.provider) {\n        const extraHeaders = requestOptions.extra_headers\n          ? (requestOptions.extra_headers as Record<string, string>)\n          : {};\n        extraHeaders['X-LiveKit-Inference-Provider'] = this.provider;\n        requestOptions.extra_headers = extraHeaders;\n      }\n\n      const stream = await this.client.chat.completions.create(\n        {\n          model: this.model,\n          messages,\n          tools,\n          stream: true,\n          stream_options: { include_usage: true },\n          ...requestOptions,\n        },\n        {\n          timeout: this.connOptions.timeoutMs,\n        },\n      );\n\n      for await (const chunk of stream) {\n        for (const choice of chunk.choices) {\n          if (this.abortController.signal.aborted) {\n            break;\n          }\n          const chatChunk = this.parseChoice(chunk.id, choice);\n          if (chatChunk) {\n            retryable = false;\n            this.queue.put(chatChunk);\n          }\n        }\n\n        if (chunk.usage) {\n          const usage = chunk.usage;\n          retryable = false;\n          this.queue.put({\n            id: chunk.id,\n            usage: {\n              completionTokens: usage.completion_tokens,\n              promptTokens: usage.prompt_tokens,\n              promptCachedTokens: usage.prompt_tokens_details?.cached_tokens || 0,\n              totalTokens: usage.total_tokens,\n            },\n          });\n        }\n      }\n    } catch (error) {\n      if (error instanceof OpenAI.APIConnectionTimeoutError) {\n        throw new APITimeoutError({ options: { retryable } });\n      } else if (error instanceof OpenAI.APIError) {\n        throw new APIStatusError({\n          message: error.message,\n          options: {\n            statusCode: error.status,\n            body: error.error,\n            requestId: error.request_id,\n            retryable,\n          },\n        });\n      } else {\n        throw new APIConnectionError({\n          message: toError(error).message,\n          options: { retryable },\n        });\n      }\n    } finally {\n      this.queue.close();\n    }\n  }\n\n  private parseChoice(\n    id: string,\n    choice: OpenAI.ChatCompletionChunk.Choice,\n  ): llm.ChatChunk | undefined {\n    const delta = choice.delta;\n\n    // https://github.com/livekit/agents/issues/688\n    // the delta can be None when using Azure OpenAI (content filtering)\n    if (delta === undefined) return undefined;\n\n    if (delta.tool_calls) {\n      // check if we have functions to calls\n      for (const tool of delta.tool_calls) {\n        if (!tool.function) {\n          continue; // oai may add other tools in the future\n        }\n\n        /**\n         * The way OpenAI streams tool calls is a bit tricky.\n         *\n         * For any new tool call, it first emits a delta tool call with id, and function name,\n         * the rest of the delta chunks will only stream the remaining arguments string,\n         * until a new tool call is started or the tool call is finished.\n         * See below for an example.\n         *\n         * Choice(delta=ChoiceDelta(content=None, function_call=None, refusal=None, role='assistant', tool_calls=None), finish_reason=None, index=0, logprobs=None)\n         * [ChoiceDeltaToolCall(index=0, id='call_LaVeHWUHpef9K1sd5UO8TtLg', function=ChoiceDeltaToolCallFunction(arguments='', name='get_weather'), type='function')]\n         * [ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='{\"location\": \"P', name=None), type=None)]\n         * [ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='aris}', name=None), type=None)]\n         * [ChoiceDeltaToolCall(index=1, id='call_ThU4OmMdQXnnVmpXGOCknXIB', function=ChoiceDeltaToolCallFunction(arguments='', name='get_weather'), type='function')]\n         * [ChoiceDeltaToolCall(index=1, id=None, function=ChoiceDeltaToolCallFunction(arguments='{\"location\": \"T', name=None), type=None)]\n         * [ChoiceDeltaToolCall(index=1, id=None, function=ChoiceDeltaToolCallFunction(arguments='okyo', name=None), type=None)]\n         * Choice(delta=ChoiceDelta(content=None, function_call=None, refusal=None, role=None, tool_calls=None), finish_reason='tool_calls', index=0, logprobs=None)\n         */\n        let callChunk: llm.ChatChunk | undefined;\n        // If we have a previous tool call and this is a new one, emit the previous\n        if (this.toolCallId && tool.id && tool.index !== this.toolIndex) {\n          callChunk = this.createRunningToolCallChunk(id, delta);\n          this.toolCallId = this.fncName = this.fncRawArguments = undefined;\n        }\n\n        // Start or continue building the current tool call\n        if (tool.function.name) {\n          this.toolIndex = tool.index;\n          this.toolCallId = tool.id;\n          this.fncName = tool.function.name;\n          this.fncRawArguments = tool.function.arguments || '';\n        } else if (tool.function.arguments) {\n          this.fncRawArguments = (this.fncRawArguments || '') + tool.function.arguments;\n        }\n\n        if (callChunk) {\n          return callChunk;\n        }\n      }\n    }\n\n    // If we're done with tool calls, emit the final one\n    if (\n      choice.finish_reason &&\n      ['tool_calls', 'stop'].includes(choice.finish_reason) &&\n      this.toolCallId !== undefined\n    ) {\n      const callChunk = this.createRunningToolCallChunk(id, delta);\n      this.toolCallId = this.fncName = this.fncRawArguments = undefined;\n      return callChunk;\n    }\n\n    // Regular content message\n    if (!delta.content) {\n      return undefined;\n    }\n\n    return {\n      id,\n      delta: {\n        role: 'assistant',\n        content: delta.content,\n      },\n    };\n  }\n\n  private createRunningToolCallChunk(\n    id: string,\n    delta: OpenAI.Chat.Completions.ChatCompletionChunk.Choice.Delta,\n  ): llm.ChatChunk {\n    return {\n      id,\n      delta: {\n        role: 'assistant',\n        content: delta.content || undefined,\n        toolCalls: [\n          llm.FunctionCall.create({\n            callId: this.toolCallId || '',\n            name: this.fncName || '',\n            args: this.fncRawArguments || '',\n          }),\n        ],\n      },\n    };\n  }\n}\n"],"mappings":"AAGA,OAAO,YAAY;AACnB;AAAA,EACE;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,OACK;AACP,YAAY,SAAS;AAErB,SAAyB,yBAAyB;AAElD,MAAM,mBAAmB;AAmFlB,MAAM,YAAY,IAAI,IAAI;AAAA,EACvB;AAAA,EACA;AAAA,EAER,YAAY,MAOT;AACD,UAAM;AAEN,UAAM,EAAE,OAAO,UAAU,SAAS,QAAQ,WAAW,aAAa,IAAI;AAEtE,UAAM,YAAY,WAAW,QAAQ,IAAI,yBAAyB;AAClE,UAAM,WAAW,UAAU,QAAQ,IAAI,6BAA6B,QAAQ,IAAI;AAChF,QAAI,CAAC,UAAU;AACb,YAAM,IAAI,MAAM,wDAAwD;AAAA,IAC1E;AAEA,UAAM,cACJ,aAAa,QAAQ,IAAI,gCAAgC,QAAQ,IAAI;AACvE,QAAI,CAAC,aAAa;AAChB,YAAM,IAAI,MAAM,iEAAiE;AAAA,IACnF;AAEA,SAAK,OAAO;AAAA,MACV;AAAA,MACA;AAAA,MACA,SAAS;AAAA,MACT,QAAQ;AAAA,MACR,WAAW;AAAA,MACX,cAAc,gBAAgB,CAAC;AAAA,IACjC;AAEA,SAAK,SAAS,IAAI,OAAO;AAAA,MACvB,SAAS,KAAK,KAAK;AAAA,MACnB,QAAQ;AAAA;AAAA,MACR,SAAS;AAAA,IACX,CAAC;AAAA,EACH;AAAA,EAEA,QAAgB;AACd,WAAO;AAAA,EACT;AAAA,EAEA,IAAI,QAAgB;AAClB,WAAO,KAAK,KAAK;AAAA,EACnB;AAAA,EAEA,OAAO,gBAAgB,aAA0B;AAC/C,WAAO,IAAI,IAAI,EAAE,OAAO,YAAY,CAAC;AAAA,EACvC;AAAA,EAEA,KAAK;AAAA,IACH;AAAA,IACA;AAAA,IACA,cAAc;AAAA,IACd;AAAA,IACA;AAAA;AAAA,IAEA;AAAA,EACF,GAQc;AACZ,QAAI,eAAwC,EAAE,GAAI,eAAe,CAAC,EAAG;AAErE,wBACE,sBAAsB,SAClB,oBACA,KAAK,KAAK,aAAa;AAE7B,QAAI,WAAW,OAAO,KAAK,OAAO,EAAE,SAAS,KAAK,sBAAsB,QAAW;AACjF,mBAAa,sBAAsB;AAAA,IACrC;AAEA,iBAAa,eAAe,SAAY,aAAa,KAAK,KAAK,aAAa;AAC5E,QAAI,YAAY;AACd,mBAAa,cAAc;AAAA,IAC7B;AAIA,mBAAe,EAAE,GAAG,cAAc,GAAG,KAAK,KAAK,aAAa;AAE5D,WAAO,IAAI,UAAU,MAAM;AAAA,MACzB,OAAO,KAAK,KAAK;AAAA,MACjB,UAAU,KAAK,KAAK;AAAA,MACpB,QAAQ,KAAK;AAAA,MACb;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA,gBAAgB;AAAA,QACd,QAAQ,KAAK,KAAK;AAAA,QAClB,WAAW,KAAK,KAAK;AAAA,MACvB;AAAA,IACF,CAAC;AAAA,EACH;AACF;AAEO,MAAM,kBAAkB,IAAI,UAAU;AAAA,EACnC;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EAEA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EAER,YACEA,MACA;AAAA,IACE;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,EACF,GAWA;AACA,UAAMA,MAAK,EAAE,SAAS,SAAS,YAAY,CAAC;AAC5C,SAAK,SAAS;AACd,SAAK,iBAAiB;AACtB,SAAK,WAAW;AAChB,SAAK,cAAc,eAAe;AAClC,SAAK,eAAe;AACpB,SAAK,QAAQ;AAAA,EACf;AAAA,EAEA,MAAgB,MAAqB;AA7PvC;AAgQI,QAAI,YAAY;AAChB,SAAK,aAAa,KAAK,UAAU,KAAK,kBAAkB,KAAK,YAAY;AAEzE,QAAI;AACF,YAAM,WAAY,MAAM,KAAK,QAAQ;AAAA,QACnC,KAAK;AAAA,MACP;AAEA,YAAM,QAAQ,KAAK,UACf,OAAO,QAAQ,KAAK,OAAO,EAAE,IAAI,CAAC,CAAC,MAAM,IAAI,OAAO;AAAA,QAClD,MAAM;AAAA,QACN,UAAU;AAAA,UACR;AAAA,UACA,aAAa,KAAK;AAAA,UAClB,YAAY,IAAI;AAAA,YACd,KAAK;AAAA,UACP;AAAA,QACF;AAAA,MACF,EAAE,IACF;AAEJ,YAAM,iBAA0C,EAAE,GAAG,KAAK,aAAa;AACvE,UAAI,CAAC,OAAO;AACV,eAAO,eAAe;AAAA,MACxB;AAGA,UAAI,KAAK,gBAAgB;AACvB,aAAK,OAAO,SAAS,MAAM;AAAA,UACzB,KAAK,eAAe;AAAA,UACpB,KAAK,eAAe;AAAA,QACtB;AAAA,MACF;AAEA,UAAI,KAAK,UAAU;AACjB,cAAM,eAAe,eAAe,gBAC/B,eAAe,gBAChB,CAAC;AACL,qBAAa,8BAA8B,IAAI,KAAK;AACpD,uBAAe,gBAAgB;AAAA,MACjC;AAEA,YAAM,SAAS,MAAM,KAAK,OAAO,KAAK,YAAY;AAAA,QAChD;AAAA,UACE,OAAO,KAAK;AAAA,UACZ;AAAA,UACA;AAAA,UACA,QAAQ;AAAA,UACR,gBAAgB,EAAE,eAAe,KAAK;AAAA,UACtC,GAAG;AAAA,QACL;AAAA,QACA;AAAA,UACE,SAAS,KAAK,YAAY;AAAA,QAC5B;AAAA,MACF;AAEA,uBAAiB,SAAS,QAAQ;AAChC,mBAAW,UAAU,MAAM,SAAS;AAClC,cAAI,KAAK,gBAAgB,OAAO,SAAS;AACvC;AAAA,UACF;AACA,gBAAM,YAAY,KAAK,YAAY,MAAM,IAAI,MAAM;AACnD,cAAI,WAAW;AACb,wBAAY;AACZ,iBAAK,MAAM,IAAI,SAAS;AAAA,UAC1B;AAAA,QACF;AAEA,YAAI,MAAM,OAAO;AACf,gBAAM,QAAQ,MAAM;AACpB,sBAAY;AACZ,eAAK,MAAM,IAAI;AAAA,YACb,IAAI,MAAM;AAAA,YACV,OAAO;AAAA,cACL,kBAAkB,MAAM;AAAA,cACxB,cAAc,MAAM;AAAA,cACpB,sBAAoB,WAAM,0BAAN,mBAA6B,kBAAiB;AAAA,cAClE,aAAa,MAAM;AAAA,YACrB;AAAA,UACF,CAAC;AAAA,QACH;AAAA,MACF;AAAA,IACF,SAAS,OAAO;AACd,UAAI,iBAAiB,OAAO,2BAA2B;AACrD,cAAM,IAAI,gBAAgB,EAAE,SAAS,EAAE,UAAU,EAAE,CAAC;AAAA,MACtD,WAAW,iBAAiB,OAAO,UAAU;AAC3C,cAAM,IAAI,eAAe;AAAA,UACvB,SAAS,MAAM;AAAA,UACf,SAAS;AAAA,YACP,YAAY,MAAM;AAAA,YAClB,MAAM,MAAM;AAAA,YACZ,WAAW,MAAM;AAAA,YACjB;AAAA,UACF;AAAA,QACF,CAAC;AAAA,MACH,OAAO;AACL,cAAM,IAAI,mBAAmB;AAAA,UAC3B,SAAS,QAAQ,KAAK,EAAE;AAAA,UACxB,SAAS,EAAE,UAAU;AAAA,QACvB,CAAC;AAAA,MACH;AAAA,IACF,UAAE;AACA,WAAK,MAAM,MAAM;AAAA,IACnB;AAAA,EACF;AAAA,EAEQ,YACN,IACA,QAC2B;AAC3B,UAAM,QAAQ,OAAO;AAIrB,QAAI,UAAU,OAAW,QAAO;AAEhC,QAAI,MAAM,YAAY;AAEpB,iBAAW,QAAQ,MAAM,YAAY;AACnC,YAAI,CAAC,KAAK,UAAU;AAClB;AAAA,QACF;AAmBA,YAAI;AAEJ,YAAI,KAAK,cAAc,KAAK,MAAM,KAAK,UAAU,KAAK,WAAW;AAC/D,sBAAY,KAAK,2BAA2B,IAAI,KAAK;AACrD,eAAK,aAAa,KAAK,UAAU,KAAK,kBAAkB;AAAA,QAC1D;AAGA,YAAI,KAAK,SAAS,MAAM;AACtB,eAAK,YAAY,KAAK;AACtB,eAAK,aAAa,KAAK;AACvB,eAAK,UAAU,KAAK,SAAS;AAC7B,eAAK,kBAAkB,KAAK,SAAS,aAAa;AAAA,QACpD,WAAW,KAAK,SAAS,WAAW;AAClC,eAAK,mBAAmB,KAAK,mBAAmB,MAAM,KAAK,SAAS;AAAA,QACtE;AAEA,YAAI,WAAW;AACb,iBAAO;AAAA,QACT;AAAA,MACF;AAAA,IACF;AAGA,QACE,OAAO,iBACP,CAAC,cAAc,MAAM,EAAE,SAAS,OAAO,aAAa,KACpD,KAAK,eAAe,QACpB;AACA,YAAM,YAAY,KAAK,2BAA2B,IAAI,KAAK;AAC3D,WAAK,aAAa,KAAK,UAAU,KAAK,kBAAkB;AACxD,aAAO;AAAA,IACT;AAGA,QAAI,CAAC,MAAM,SAAS;AAClB,aAAO;AAAA,IACT;AAEA,WAAO;AAAA,MACL;AAAA,MACA,OAAO;AAAA,QACL,MAAM;AAAA,QACN,SAAS,MAAM;AAAA,MACjB;AAAA,IACF;AAAA,EACF;AAAA,EAEQ,2BACN,IACA,OACe;AACf,WAAO;AAAA,MACL;AAAA,MACA,OAAO;AAAA,QACL,MAAM;AAAA,QACN,SAAS,MAAM,WAAW;AAAA,QAC1B,WAAW;AAAA,UACT,IAAI,aAAa,OAAO;AAAA,YACtB,QAAQ,KAAK,cAAc;AAAA,YAC3B,MAAM,KAAK,WAAW;AAAA,YACtB,MAAM,KAAK,mBAAmB;AAAA,UAChC,CAAC;AAAA,QACH;AAAA,MACF;AAAA,IACF;AAAA,EACF;AACF;","names":["llm"]}